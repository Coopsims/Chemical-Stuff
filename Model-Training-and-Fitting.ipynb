{
 "cells": [
  {
   "cell_type": "code",
   "id": "8508c25c15961501",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-24T02:46:59.150143Z",
     "start_time": "2024-04-24T02:46:52.688048Z"
    }
   },
   "source": [
    "# Imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import csv\n",
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "import config\n",
    "from rdkit.DataStructs import TanimotoSimilarity\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy.sparse import vstack, hstack, csr_matrix\n",
    "from rdkit.Chem import Draw\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import xgboost as xgb\n",
    "import torch\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             classification_report, confusion_matrix, roc_curve, auc, average_precision_score)"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T02:46:59.164997Z",
     "start_time": "2024-04-24T02:46:59.152146Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def connect_to_database():\n",
    "    \"\"\"Connect to the MySQL database using settings from the config module.\"\"\"\n",
    "    try:\n",
    "        print(\"Connecting to the MySQL database...\")\n",
    "        conn = mysql.connector.connect(**config.DATABASE_CONFIG)\n",
    "        if conn.is_connected():\n",
    "            print(\"Connection established.\")\n",
    "        else:\n",
    "            print(\"Connection failed.\")\n",
    "        return conn\n",
    "    except Error as e:\n",
    "        print(f\"The error '{e}' occurred\")\n",
    "        return None\n",
    "\n",
    "def close_connection(conn):\n",
    "    \"\"\"Close the database connection.\"\"\"\n",
    "    if conn.is_connected():\n",
    "        conn.close()\n",
    "        print(\"The connection is closed.\")\n",
    "\n",
    "def fetch_protein_mapping(cursor):\n",
    "    query = \"SELECT protein_name, protein_numeric FROM protein_mapping\"\n",
    "    cursor.execute(query)\n",
    "    mapping_data = cursor.fetchall()\n",
    "    print('fetched protein mapping...')\n",
    "    return {name: num for name, num in mapping_data}"
   ],
   "id": "383fc40c4c1a4c90",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Getting CSV Ready for Training",
   "id": "496e6cbdcb7a38c3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_df = pd.read_csv('Cleaned-csv/train_data_strings.csv')\n",
    "test_df = pd.read_csv('Cleaned-csv/test_data_strings.csv')"
   ],
   "id": "7bfe554502c925ca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_df['molVector'] = train_df['molVector'].apply(lambda x: [int(i) for i in x.split(',')])\n",
    "test_df['molVector'] = test_df['molVector'].apply(lambda x: [int(i) for i in x.split(',')])"
   ],
   "id": "fd1855cbab7f54c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "first_row_vector = train_df['molVector'].iloc[0]\n",
    "max_len = len(first_row_vector)\n",
    "print(max_len)"
   ],
   "id": "c837379f9f2d5429",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"about 6 minutes for this code\"\"\"\n",
    "molVector_train_df = pd.DataFrame(train_df['molVector'].to_list(), columns=[f\"molVect{i + 1}\" for i in range(max_len)])\n",
    "molVector_test_df = pd.DataFrame(test_df['molVector'].to_list(), columns=[f\"molVect{i + 1}\" for i in range(max_len)])"
   ],
   "id": "68a084f124122c20",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(molVector_train_df.dtypes)",
   "id": "96ca5450c32aa652",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"Takes around 12 minutes for this code\"\"\"\n",
    "for col in molVector_train_df.columns:\n",
    "    molVector_train_df[col] = molVector_train_df[col].astype(np.int8)\n",
    "print('Changed type to int8...')\n",
    "train_df = pd.concat([train_df.drop(['molVector'], axis=1), molVector_train_df], axis=1)\n",
    "print('combined training set...')\n",
    "\n",
    "for col in molVector_test_df.columns:\n",
    "    molVector_test_df[col] = molVector_test_df[col].astype(np.int8)\n",
    "print('Changed type to int8...')\n",
    "test_df = pd.concat([test_df.drop(['molVector'], axis=1), molVector_test_df], axis=1)\n",
    "print('combined testing set...')\n"
   ],
   "id": "c1ad8c79f0cf5121",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_df.to_csv('Cleaned-csv/Cleaned_train_df-512bit-2rad.csv', index=False)\n",
    "test_df.to_csv('Cleaned-csv/Cleaned_test_df-512bit-2rad.csv', index=False)"
   ],
   "id": "5a0edfeb4784d6fd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load Cleaned CSV",
   "id": "b6f646db73f8a4ff"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T22:20:58.804632Z",
     "start_time": "2024-04-23T22:19:44.023269Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_df = pd.read_csv('Cleaned-csv/Cleaned_train_df-512bit-2rad.csv')\n",
    "test_df = pd.read_csv('Cleaned-csv/Cleaned_test_df-512bit-2rad.csv')"
   ],
   "id": "ecc8bad712d9ae1c",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T22:21:19.596380Z",
     "start_time": "2024-04-23T22:21:19.581872Z"
    }
   },
   "cell_type": "code",
   "source": "print(train_df.dtypes)",
   "id": "ea55f8767cf36e25",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                 int64\n",
      "binds              int64\n",
      "Protein_numeric    int64\n",
      "molVect1           int64\n",
      "molVect2           int64\n",
      "                   ...  \n",
      "molVect508         int64\n",
      "molVect509         int64\n",
      "molVect510         int64\n",
      "molVect511         int64\n",
      "molVect512         int64\n",
      "Length: 515, dtype: object\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T22:21:36.792574Z",
     "start_time": "2024-04-23T22:21:31.623473Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Drop 'id' column from training and testing data\n",
    "train_df = train_df.drop(columns=['id'])\n",
    "test_df = test_df.drop(columns=['id'])\n",
    "\n",
    "# Define target variable for training data\n",
    "y_train = train_df['binds']\n",
    "X_train = train_df.drop(columns=['binds'])\n",
    "\n",
    "# Define target variable for testing data (if applicable)\n",
    "y_test = test_df['binds']\n",
    "X_test = test_df.drop(columns=['binds'])\n"
   ],
   "id": "2a52a5bb88263be2",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T22:21:55.429685Z",
     "start_time": "2024-04-23T22:21:55.354495Z"
    }
   },
   "cell_type": "code",
   "source": [
    "del train_df, test_df\n",
    "gc.collect()"
   ],
   "id": "fd35c8e5f32bab96",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# XGBoost Training",
   "id": "c009fee7fcf48b3e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T22:23:06.326733Z",
     "start_time": "2024-04-23T22:22:51.068125Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create XGBoost specific DMatrix data format\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)"
   ],
   "id": "44f18d2985621c22",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T22:23:55.931452Z",
     "start_time": "2024-04-23T22:23:28.251576Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set parameters\n",
    "params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'n_estimators': 1000,  \n",
    "    'learning_rate':0.005,\n",
    "    'alpha': 10,  \n",
    "    'lambda': 1,  \n",
    "    'max_depth': 3,\n",
    "    'min_child_weight': 50,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "# Train model\n",
    "model = xgb.train(params, dtrain)"
   ],
   "id": "635d3ab467f92d8c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:23:28] WARNING: C:\\b\\abs_7diruzi3as\\croot\\xgboost-split_1712794727514\\work\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T22:24:24.351830Z",
     "start_time": "2024-04-23T22:24:24.339315Z"
    }
   },
   "cell_type": "code",
   "source": "model.save_model('Trained-Models/xgb_model-2-0.json')",
   "id": "494a70ee1b1bb55c",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Neural Network",
   "id": "42b8373e16c79a9e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T22:25:15.905451Z",
     "start_time": "2024-04-23T22:25:13.707251Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(\"CUDA Available: \", cuda_available)\n",
    "\n",
    "# Get the number of GPUs available\n",
    "if cuda_available:\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(\"Number of GPUs Available: \", num_gpus)\n",
    "    for i in range(num_gpus):\n",
    "        print(\"GPU \", i, \": \", torch.cuda.get_device_name(i))\n",
    "else:\n",
    "    print(\"No GPU available, using CPU instead.\")\n"
   ],
   "id": "78432d2478668b18",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available:  True\n",
      "Number of GPUs Available:  1\n",
      "GPU  0 :  NVIDIA GeForce RTX 3070\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T02:47:41.324420Z",
     "start_time": "2024-04-24T02:47:41.314975Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "id": "65e9d6ed044f35a",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T22:45:53.128692Z",
     "start_time": "2024-04-23T22:45:52.091928Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Assuming 'X_train', 'X_test' are DataFrames or Series and 'y_train', 'y_test' are Series\n",
    "X_train_np = X_train.values  # Convert DataFrame to NumPy array\n",
    "y_train_np = y_train.values  # Convert Series to NumPy array\n",
    "X_test_np = X_test.values    # Convert DataFrame to NumPy array\n",
    "y_test_np = y_test.values    # Convert Series to NumPy array\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_np, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_np, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_np, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test_np, dtype=torch.float32)"
   ],
   "id": "229b48bd7034e46",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-04-24T01:16:43.027572Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Define the dataset class\n",
    "class BinaryDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Ensure that tensors returned by __getitem__ are of type FloatTensor\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(BinaryClassifier, self).__init__()\n",
    "        # More nuanced layer design\n",
    "        self.layer1 = nn.Linear(input_size, 512)\n",
    "        self.norm1 = nn.BatchNorm1d(512)\n",
    "        self.act1 = nn.LeakyReLU(negative_slope=0.01)  # LeakyReLU to prevent dying neurons\n",
    "        self.dropout1 = nn.Dropout(0.3)  # Slightly lower dropout for this layer\n",
    "\n",
    "        self.layer2 = nn.Linear(512, 256)\n",
    "        self.norm2 = nn.BatchNorm1d(256)\n",
    "        self.act2 = nn.LeakyReLU(negative_slope=0.01)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "\n",
    "        self.layer3 = nn.Linear(256, 128)\n",
    "        self.norm3 = nn.BatchNorm1d(128)\n",
    "        self.act3 = nn.LeakyReLU(negative_slope=0.01)\n",
    "        self.dropout3 = nn.Dropout(0.4)  # Increased dropout for deeper layers\n",
    "\n",
    "        self.layer4 = nn.Linear(128, 64)\n",
    "        self.norm4 = nn.BatchNorm1d(64)\n",
    "        self.act4 = nn.LeakyReLU(negative_slope=0.01)\n",
    "        self.dropout4 = nn.Dropout(0.4)\n",
    "\n",
    "        self.output_layer = nn.Linear(64, 1)\n",
    "        self.output_act = nn.Sigmoid()  # Output layer activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.norm1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.layer2(x)\n",
    "        x = self.norm2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = self.layer3(x)\n",
    "        x = self.norm3(x)\n",
    "        x = self.act3(x)\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        x = self.layer4(x)\n",
    "        x = self.norm4(x)\n",
    "        x = self.act4(x)\n",
    "        x = self.dropout4(x)\n",
    "\n",
    "        x = self.output_layer(x)\n",
    "        x = self.output_act(x)\n",
    "        return x\n",
    "\n",
    "# Assuming X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor are already defined\n",
    "# Create dataset instances\n",
    "train_dataset = BinaryDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = BinaryDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create DataLoader instances for batch processing and shuffling\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize the model and move it to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BinaryClassifier(input_size=513).to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "# Training and validation functions\n",
    "def train_model(model, train_loader, criterion, optimizer, epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # Move data to GPU\n",
    "            optimizer.zero_grad()  # Clear gradients\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels.unsqueeze(1))  # Calculate loss\n",
    "            loss.backward()  # Backpropagation\n",
    "            optimizer.step()  # Update weights\n",
    "            total_loss += loss.item()  # Accumulate loss\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f'Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}')\n",
    "        validate_model(model, test_loader, criterion)\n",
    "\n",
    "def validate_model(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    all_probabilities = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels.unsqueeze(1))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Collect labels and predictions for metrics calculation\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            probabilities = outputs.cpu().numpy()  # Assuming sigmoid output\n",
    "            all_probabilities.extend(probabilities)\n",
    "            predicted = (probabilities > 0.5).astype(int)\n",
    "            all_predictions.extend(predicted.squeeze())\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    precision = precision_score(all_labels, all_predictions)\n",
    "    recall = recall_score(all_labels, all_predictions)\n",
    "    f1 = f1_score(all_labels, all_predictions)\n",
    "    average_precision = average_precision_score(all_labels, all_probabilities)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Validation Loss: {total_loss / len(test_loader):.4f}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"Mean Average Precision: {average_precision:.4f}\")\n",
    "\n",
    "    # Confusion Matrix\n",
    "    conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "    print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "\n",
    "# Run training\n",
    "train_model(model, train_loader, criterion, optimizer, 10)  # Train for 100 epochs"
   ],
   "id": "247525da8e3be871",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.0144\n",
      "Validation Loss: 29.0437\n",
      "Accuracy: 0.5800\n",
      "Precision: 0.5436\n",
      "Recall: 0.9979\n",
      "F1 Score: 0.7038\n",
      "Average Precision: 0.6287\n",
      "Confusion Matrix:\n",
      " [[ 95593 494313]\n",
      " [  1266 588640]]\n",
      "Epoch 2/10, Loss: 0.0111\n",
      "Validation Loss: 11.7015\n",
      "Accuracy: 0.5740\n",
      "Precision: 0.5401\n",
      "Recall: 0.9972\n",
      "F1 Score: 0.7007\n",
      "Average Precision: 0.7523\n",
      "Confusion Matrix:\n",
      " [[ 88966 500940]\n",
      " [  1675 588231]]\n",
      "Epoch 3/10, Loss: 0.0106\n",
      "Validation Loss: 4.8915\n",
      "Accuracy: 0.5756\n",
      "Precision: 0.5409\n",
      "Recall: 0.9992\n",
      "F1 Score: 0.7019\n",
      "Average Precision: 0.9097\n",
      "Confusion Matrix:\n",
      " [[ 89597 500309]\n",
      " [   461 589445]]\n",
      "Epoch 4/10, Loss: 0.0103\n",
      "Validation Loss: 4.0628\n",
      "Accuracy: 0.5751\n",
      "Precision: 0.5406\n",
      "Recall: 0.9995\n",
      "F1 Score: 0.7017\n",
      "Average Precision: 0.9331\n",
      "Confusion Matrix:\n",
      " [[ 88821 501085]\n",
      " [   275 589631]]\n",
      "Epoch 5/10, Loss: 0.0102\n",
      "Validation Loss: 3.8709\n",
      "Accuracy: 0.5739\n",
      "Precision: 0.5399\n",
      "Recall: 0.9994\n",
      "F1 Score: 0.7011\n",
      "Average Precision: 0.9049\n",
      "Confusion Matrix:\n",
      " [[ 87607 502299]\n",
      " [   377 589529]]\n",
      "Epoch 6/10, Loss: 0.0100\n",
      "Validation Loss: 26.1963\n",
      "Accuracy: 0.5730\n",
      "Precision: 0.5394\n",
      "Recall: 0.9999\n",
      "F1 Score: 0.7007\n",
      "Average Precision: 0.5956\n",
      "Confusion Matrix:\n",
      " [[ 86193 503713]\n",
      " [    82 589824]]\n",
      "Epoch 7/10, Loss: 0.0100\n",
      "Validation Loss: 4.0582\n",
      "Accuracy: 0.5730\n",
      "Precision: 0.5394\n",
      "Recall: 0.9986\n",
      "F1 Score: 0.7005\n",
      "Average Precision: 0.8949\n",
      "Confusion Matrix:\n",
      " [[ 86911 502995]\n",
      " [   823 589083]]\n",
      "Epoch 8/10, Loss: 0.0099\n",
      "Validation Loss: 6.3060\n",
      "Accuracy: 0.5731\n",
      "Precision: 0.5394\n",
      "Recall: 0.9993\n",
      "F1 Score: 0.7007\n",
      "Average Precision: 0.8538\n",
      "Confusion Matrix:\n",
      " [[ 86607 503299]\n",
      " [   387 589519]]\n",
      "Epoch 9/10, Loss: 0.0098\n",
      "Validation Loss: 4.7776\n",
      "Accuracy: 0.5733\n",
      "Precision: 0.5396\n",
      "Recall: 0.9999\n",
      "F1 Score: 0.7009\n",
      "Average Precision: 0.9172\n",
      "Confusion Matrix:\n",
      " [[ 86514 503392]\n",
      " [    30 589876]]\n",
      "Epoch 10/10, Loss: 0.0097\n",
      "Validation Loss: 7.1557\n",
      "Accuracy: 0.5731\n",
      "Precision: 0.5395\n",
      "Recall: 0.9996\n",
      "F1 Score: 0.7008\n",
      "Average Precision: 0.8394\n",
      "Confusion Matrix:\n",
      " [[ 86543 503363]\n",
      " [   251 589655]]\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T01:47:16.046884Z",
     "start_time": "2024-04-24T01:47:16.024838Z"
    }
   },
   "cell_type": "code",
   "source": "torch.save(model, 'Trained-Models/Torch-model1-1.pth')",
   "id": "1c58da58fe7c6dd0",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T03:07:55.413765Z",
     "start_time": "2024-04-24T02:47:53.650554Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define a dataset class for PyTorch\n",
    "class MolecularDataset(Dataset):\n",
    "    def __init__(self, features, labels=None):\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        if labels is not None:\n",
    "            self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "        else:\n",
    "            self.labels = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.labels is not None:\n",
    "            return self.features[idx], self.labels[idx]\n",
    "        else:\n",
    "            return self.features[idx]\n",
    "\n",
    "# Convert SMILES to fingerprint\n",
    "def smiles_to_fingerprint(smiles, radius=2, nBits=512):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol:\n",
    "        return list(AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits))\n",
    "    else:\n",
    "        return [0]*nBits\n",
    "\n",
    "# Process SMILES strings in batches\n",
    "def process_in_batches(smiles_series, batch_size=1000):\n",
    "    results = []\n",
    "    for i in tqdm(range(0, len(smiles_series), batch_size), desc=\"Processing Batches\"):\n",
    "        batch = smiles_series[i:i+batch_size].apply(smiles_to_fingerprint)\n",
    "        results.extend(batch)\n",
    "    return results\n",
    "\n",
    "# Load and process the data\n",
    "test_submission_data = pd.read_csv('leash-BELKA/test.csv')\n",
    "test_submission_df = pd.DataFrame()\n",
    "test_submission_df['molecule_smiles'] = process_in_batches(test_submission_data['molecule_smiles'], batch_size=500)"
   ],
   "id": "21a93d855e1c7a08",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T03:08:52.309975Z",
     "start_time": "2024-04-24T03:08:18.701227Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Connect to DB and fetch protein mappings\n",
    "conn = connect_to_database()\n",
    "cursor = conn.cursor()\n",
    "protein_mapping = fetch_protein_mapping(cursor)\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "# Map protein names to numeric\n",
    "test_submission_df['Protein_numeric'] = test_submission_data['protein_name'].map(protein_mapping)\n",
    "\n",
    "# Create dataset\n",
    "features = np.column_stack((test_submission_df['molecule_smiles'].tolist(), test_submission_df['Protein_numeric'].values))\n",
    "test_dataset = MolecularDataset(features)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ],
   "id": "bbeedb1faa4b44e6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to the MySQL database...\n",
      "Connection established.\n",
      "fetched protein mapping...\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T03:11:26.363746Z",
     "start_time": "2024-04-24T03:11:26.349855Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "7d8fce734c1fc8ba",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T03:11:53.419074Z",
     "start_time": "2024-04-24T03:11:27.421714Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load PyTorch model and ensure it is on the correct device\n",
    "model = torch.load('Trained-Models/Torch-model1-1.pth')\n",
    "model.to(device)\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "# Predict function for PyTorch model\n",
    "def predict(model, dataloader):\n",
    "    predictions = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            data = data.to(device)  # Move data to the same device as the model\n",
    "            outputs = model(data)\n",
    "            predicted_probs = torch.sigmoid(outputs).cpu().numpy()  # Convert outputs to probabilities and move to CPU\n",
    "            predictions.extend(predicted_probs.flatten())\n",
    "    return predictions\n",
    "\n",
    "# Perform predictions\n",
    "predictions = predict(model, test_loader)\n",
    "\n",
    "# Save predictions\n",
    "test_submission_data['binds'] = predictions\n",
    "test_submission_data[['id', 'binds']].to_csv('Submission-Files/Submission2Pytorch.csv', index=False)"
   ],
   "id": "6b910d86dd7b6c91",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model Results",
   "id": "6adc90fb8acba23d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T22:24:50.732675Z",
     "start_time": "2024-04-23T22:24:41.232273Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Make the prediction\n",
    "raw_predictions_train = model.predict(dtrain)\n",
    "\n",
    "# Create a dataframe with the predicted probabilities\n",
    "probabilities_train = pd.DataFrame(raw_predictions_train)\n",
    "print(probabilities_train.head())\n",
    "\n",
    "# Convert probabilities into binary output\n",
    "predictions_train = [1 if proba > 0.5 else 0 for proba in raw_predictions_train]\n",
    "\n",
    "# Now print the accuracy with these classes\n",
    "print(\"Accuracy:\", accuracy_score(y_train, predictions_train))\n",
    "print(\"Precision:\", precision_score(y_train, predictions_train))\n",
    "print(\"Recall:\", recall_score(y_train, predictions_train))\n",
    "print(\"F1 Score:\", f1_score(y_train, predictions_train))\n",
    "\n",
    "print(classification_report(y_train, predictions_train, target_names=['Class 0', 'Class 1']))\n",
    "\n",
    "conf_matrix_train = confusion_matrix(y_train, predictions_train)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix_train)\n",
    "\n",
    "fpr_train, tpr_train, thresholds_train = roc_curve(y_train, probabilities_train)\n",
    "roc_auc_train = auc(fpr_train, tpr_train)\n",
    "\n",
    "print(\"AUC Score (Training):\", roc_auc_train)\n",
    "\n",
    "# Average Precision Score\n",
    "average_precision_train = average_precision_score(y_train, probabilities_train)\n",
    "print('Mean Average Precision (Training): {0:0.2f}'.format(average_precision_train))\n"
   ],
   "id": "147c8999a8b17bf8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0\n",
      "0  0.475814\n",
      "1  0.524442\n",
      "2  0.524442\n",
      "3  0.475814\n",
      "4  0.524442\n",
      "Accuracy: 0.9973715\n",
      "Precision: 1.0\n",
      "Recall: 0.994743\n",
      "F1 Score: 0.997364572779551\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      1.00      1.00   1000000\n",
      "     Class 1       1.00      0.99      1.00   1000000\n",
      "\n",
      "    accuracy                           1.00   2000000\n",
      "   macro avg       1.00      1.00      1.00   2000000\n",
      "weighted avg       1.00      1.00      1.00   2000000\n",
      "\n",
      "Confusion Matrix:\n",
      " [[1000000       0]\n",
      " [   5257  994743]]\n",
      "AUC Score (Training): 0.9973719999999999\n",
      "Mean Average Precision (Training): 1.00\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T22:25:00.313693Z",
     "start_time": "2024-04-23T22:24:54.927711Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Make the prediction\n",
    "raw_predictions = model.predict(dtest)\n",
    "\n",
    "# Create a dataframe with the predicted probabilities  \n",
    "probabilities = pd.DataFrame(raw_predictions)\n",
    "print(probabilities.head())\n",
    "\n",
    "# Convert probabilities into binary output\n",
    "predictions = [1 if proba > 0.5 else 0 for proba in raw_predictions]\n",
    "\n",
    "# Now print the accuracy with these classes\n",
    "print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
    "print(\"Precision:\", precision_score(y_test, predictions))\n",
    "print(\"Recall:\", recall_score(y_test, predictions))\n",
    "print(\"F1 Score:\", f1_score(y_test, predictions))\n",
    "\n",
    "print(classification_report(y_test, predictions, target_names=['Class 0', 'Class 1']))\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, predictions)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, probabilities)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "print(\"AUC Score:\", roc_auc)\n",
    "\n",
    "# Average Precision Score\n",
    "average_precision = average_precision_score(y_test, probabilities)\n",
    "print('Mean Average Precision (micro): {0:0.2f}'.format(average_precision))"
   ],
   "id": "32c8896ecb885420",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0\n",
      "0  0.524442\n",
      "1  0.524442\n",
      "2  0.524442\n",
      "3  0.524442\n",
      "4  0.524412\n",
      "Accuracy: 0.5733727068380386\n",
      "Precision: 0.5396096777086548\n",
      "Recall: 0.9995694229250084\n",
      "F1 Score: 0.7008636407938935\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       1.00      0.15      0.26    589906\n",
      "     Class 1       0.54      1.00      0.70    589906\n",
      "\n",
      "    accuracy                           0.57   1179812\n",
      "   macro avg       0.77      0.57      0.48   1179812\n",
      "weighted avg       0.77      0.57      0.48   1179812\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 86820 503086]\n",
      " [   254 589652]]\n",
      "AUC Score: 0.9889508116939492\n",
      "Mean Average Precision (micro): 0.99\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Code for Creating Submission Using Model",
   "id": "3a876e7121b8a490"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "# Load CSV data\n",
    "test_submission_data = pd.read_csv('leash-BELKA/test.csv')\n",
    "\n",
    "# Function to convert SMILES string to a fingerprint\n",
    "def smiles_to_fingerprint(smiles, radius=2, nBits=512):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is not None:\n",
    "        return list(AllChem.GetMorganFingerprintAsBitVect(mol, radius=radius, nBits=nBits))\n",
    "    else:\n",
    "        return [0]*nBits  # Return a zero vector if the molecule parsing fails\n",
    "\n",
    "# Process SMILES strings in batches\n",
    "def process_in_batches(smiles_series, batch_size=1000):\n",
    "    num_batches = (len(smiles_series) + batch_size - 1) // batch_size\n",
    "    results = []\n",
    "    for i in tqdm(range(num_batches), desc=\"Processing Batches\"):\n",
    "        batch = smiles_series.iloc[i*batch_size:(i+1)*batch_size].apply(smiles_to_fingerprint)\n",
    "        results.extend(batch)\n",
    "    return pd.Series(results)\n",
    "\n",
    "# Create dataframe and process fingerprints\n",
    "test_submission_df = pd.DataFrame()\n",
    "test_submission_df['id'] = test_submission_data['id']\n",
    "test_submission_df['molecule_smiles'] = process_in_batches(test_submission_data['molecule_smiles'], batch_size=500)"
   ],
   "id": "d3c66aa197009955",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Assuming you have a way to fetch protein mappings similar to the database method shown\n",
    "\n",
    "conn = connect_to_database()\n",
    "cursor = conn.cursor()\n",
    "protein_mapping = fetch_protein_mapping(cursor)\n",
    "cursor.close()\n",
    "conn.close()"
   ],
   "id": "efdd1834c501f582",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "test_submission_df['Protein_numeric'] = test_submission_data['protein_name'].map(protein_mapping)",
   "id": "42a4775d9ec165b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Adjust based on your fingerprint length\n",
    "\"\"\"Expect about 9 minutes for this code\"\"\"\n",
    "molVector_test_df = pd.DataFrame(test_submission_df['molecule_smiles'].tolist(), columns=[f\"molVect{i + 1}\" for i in range(max_len)])\n",
    "print('converted to many columns...')\n",
    "for col in molVector_test_df.columns:\n",
    "    molVector_test_df[col] = molVector_test_df[col].astype(np.int8)\n",
    "print('Converted type to int8...')\n",
    "test_submission_df = pd.concat([test_submission_df.drop(['molecule_smiles'], axis=1), molVector_test_df], axis=1)"
   ],
   "id": "e111281775d20153",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T01:09:18.603164Z",
     "start_time": "2024-04-24T01:07:52.676158Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_submission_df['id'] = test_submission_data['id']\n",
    "test_submission_df.to_csv('Cleaned-csv/final-submission-cleaned-512bits-2rad.csv', index=False)"
   ],
   "id": "b274d0665996b4d3",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# To load model\n",
    "model = xgb.Booster()\n",
    "model.load_model('Trained-Models/xgb_model.json')"
   ],
   "id": "951c6bfef0e1add3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# Prepare data for prediction\n",
    "dtest1 = xgb.DMatrix(test_submission_df.drop(['id'], axis=1))"
   ],
   "id": "1aa1f742b7234d3f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "raw_predictions = model.predict(dtest1)\n",
    "\n",
    "# Save predictions\n",
    "test_submission_df['binds'] = raw_predictions"
   ],
   "id": "2925e1294a9ce4bd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "test_submission_df[['id', 'binds']].to_csv('Submission-Files/Submission2XGBoost.csv', index=False)",
   "id": "430e73e00f9774c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "10473071ec225741",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
