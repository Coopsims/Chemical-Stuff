{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import csv\n",
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "import config\n",
    "from rdkit.DataStructs import TanimotoSimilarity\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from rdkit.Chem import Draw"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "numCores = os.cpu_count()\n",
    "numCores"
   ],
   "id": "389e2521e4f142f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# This code below is for adding most of the data to a MySQL database",
   "id": "fe28be39a5f5a3d2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_max_id(cursor, binds_value):\n",
    "    try:\n",
    "        # Adjust the query to filter by 'binds' value\n",
    "        cursor.execute(\"SELECT MAX(id) FROM compounds WHERE binds = %s\", (binds_value,))\n",
    "        result = cursor.fetchone()\n",
    "        print(\"Max id for binds =\", binds_value, \"is:\", result[0])\n",
    "        return result[0] if result[0] is not None else 0\n",
    "    except Error as e:\n",
    "        print(\"Error fetching max id for binds =\", binds_value, \":\", e)\n",
    "        return 0\n",
    "    \n",
    "def load_csv_in_chunks(filepath, batch_size, start_id, desired_binds,cursor, conn):\n",
    "    try:\n",
    "        # Open the CSV file\n",
    "        with open(filepath, mode='r', newline='', encoding='utf-8') as file:\n",
    "            reader = csv.reader(file)\n",
    "            headers = next(reader)  # Skip the header row\n",
    "            binds_index = headers.index(\"binds\")  # Find the index of the 'binds' column\n",
    "            batch = []\n",
    "            current_id = 0  # This will track the ID of each row processed\n",
    "\n",
    "            for row in tqdm(reader, desc=\"Processing rows\"):\n",
    "                current_id = int(row[0])  # Assuming the ID is in the first column\n",
    "                if current_id <= start_id:\n",
    "                    continue  # Skip rows until we reach the starting ID\n",
    "\n",
    "                if int(row[binds_index]) == desired_binds:  # Check if 'binds' is the desired value\n",
    "                    batch.append(row)\n",
    "                    if len(batch) >= batch_size:\n",
    "                        insert_batch(batch, headers,cursor, conn)\n",
    "                        batch = []  # Reset the batch list after inserting\n",
    "\n",
    "            if batch:  # Insert any remaining rows in the final batch\n",
    "                insert_batch(batch, headers,cursor, conn)\n",
    "\n",
    "    except Exception as e:  # Use Exception to catch all possible issues\n",
    "        print(f\"Error reading file at ID {current_id}: {e}\")\n",
    "\n",
    "def insert_batch(batch, headers, cursor, conn):\n",
    "    try:\n",
    "        query = f\"INSERT INTO compounds ({', '.join(headers)}) VALUES (%s, %s, %s, %s, %s, %s, %s)\"\n",
    "        cursor.executemany(query, batch)\n",
    "        conn.commit()  # Use the connection object to commit the transaction\n",
    "        print(f\"Batch inserted. Batch size: {len(batch)} Last ID: {batch[-1][0]}\")\n",
    "    except Error as e:\n",
    "        print(f\"Failed to insert batch into MySQL table. Error: {e}\")\n",
    "        conn.rollback()  # Use the connection object to rollback the transaction\n"
   ],
   "id": "81f93c2279a027ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Creating quick functions for connecting to DB",
   "id": "a86146b569d7e0ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def connect_to_database():\n",
    "    \"\"\"Connect to the MySQL database using settings from the config module.\"\"\"\n",
    "    try:\n",
    "        print(\"Connecting to the MySQL database...\")\n",
    "        conn = mysql.connector.connect(**config.DATABASE_CONFIG)\n",
    "        if conn.is_connected():\n",
    "            print(\"Connection established.\")\n",
    "        else:\n",
    "            print(\"Connection failed.\")\n",
    "        return conn\n",
    "    except Error as e:\n",
    "        print(f\"The error '{e}' occurred\")\n",
    "        return None\n",
    "\n",
    "def close_connection(conn):\n",
    "    \"\"\"Close the database connection.\"\"\"\n",
    "    if conn.is_connected():\n",
    "        conn.close()\n",
    "        print(\"The connection is closed.\")"
   ],
   "id": "edbb1437377788a0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Actual Code that adds items to the DB\n",
    "### all rows have been added where binds == 1 so only rows yet to be added are binds == 0"
   ],
   "id": "ce08bec2c80bb020"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set do to True if you want to run this, otherwise leave False\n",
    "\n",
    "do = False\n",
    "if do:\n",
    "    \"\"\"Loads Data into the MySQL database robustly.\"\"\"\n",
    "    desired_binds = 1\n",
    "    conn = connect_to_database()\n",
    "    if conn is not None:\n",
    "        try:\n",
    "            cursor = conn.cursor()\n",
    "    \n",
    "            # Determine the maximum ID already processed\n",
    "            max_id = get_max_id(cursor, desired_binds)\n",
    "            print(f\"Starting import from ID {max_id + 1} using {desired_binds} binds\")\n",
    "            \n",
    "            # Path to your CSV file and batch size\n",
    "            csv_file_path = 'C:/Users/funkb/DataspellProjects/Chemical-Stuff/leash-BELKA/train.csv'\n",
    "            batch_size = 100000  # Modify as needed\n",
    "            \n",
    "            # Start loading from the next ID\n",
    "            load_csv_in_chunks(csv_file_path, batch_size, max_id, desired_binds, cursor, conn)\n",
    "        finally:\n",
    "            cursor.close()\n",
    "            close_connection(conn)\n"
   ],
   "id": "afc9e21ffcc8b8cd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now we check to see what the density of the bits would be using the graph below. \n",
    "Typically flatter is better, but you dont want to be too flat so 512 is looking \n",
    "to be good as that is where the slope starts to level out"
   ],
   "id": "72b8ea508d167e74"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import networkx as nx\n",
    "# Convert SMILES to molecule\n",
    "smiles = \"O=C(N[Dy])c1cccc(F)c1Nc1nc(NCc2nccs2)nc(Nc2ccc(Cn3ccnc3)cc2)n1\"\n",
    "mol = Chem.MolFromSmiles(smiles)\n",
    "Draw.MolToImage(mol)"
   ],
   "id": "97457b69368d0958",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Initialize an empty graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes with atom information\n",
    "for atom in mol.GetAtoms():\n",
    "    G.add_node(atom.GetIdx(), element=atom.GetSymbol(), atomic_num=atom.GetAtomicNum())\n",
    "\n",
    "# Add edges with bond information\n",
    "for bond in mol.GetBonds():\n",
    "    G.add_edge(bond.GetBeginAtomIdx(), bond.GetEndAtomIdx(), bond_type=bond.GetBondType())\n",
    "\n",
    "# Generate positions for the nodes using spring layout\n",
    "pos = nx.spring_layout(G)\n",
    "\n",
    "# Draw nodes\n",
    "nx.draw_networkx_nodes(G, pos, node_size=100, node_color='skyblue')\n",
    "\n",
    "# Draw edges\n",
    "nx.draw_networkx_edges(G, pos, width=6)\n",
    "\n",
    "# Draw node labels\n",
    "node_labels = {node: data['element'] for node, data in G.nodes(data=True)}\n",
    "nx.draw_networkx_labels(G, pos, labels=node_labels, font_size=12, font_color='darkred')\n",
    "\n",
    "# Draw edge labels\n",
    "edge_labels = {(u, v): f\"{d['bond_type']}\" for u, v, d in G.edges(data=True)}\n",
    "nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_color='green')\n",
    "\n",
    "# Set the title and turn off the axis\n",
    "plt.title('Graph Representation of Ethanol (CCO)')\n",
    "plt.axis('off')  # Turn off the axis\n",
    "plt.show()\n",
    "# # Generate fingerprints with different bit sizes\n",
    "# bit_sizes = [2,25,30,50,75,100,256, 512, 1024, 2048, 4096]\n",
    "# fingerprints = [AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=bits) for bits in bit_sizes]\n",
    "# \n",
    "# # Analyze bit density (proportion of bits that are set to 1)\n",
    "# bit_densities = [sum(fp) / len(fp) for fp in fingerprints]\n",
    "# \n",
    "# # Plotting\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(bit_sizes, bit_densities, marker='o')\n",
    "# plt.title('Fingerprint Density vs Bit Size')\n",
    "# plt.xlabel('Number of Bits')\n",
    "# plt.ylabel('Density (Proportion of Bits Set to 1)')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ],
   "id": "9e3d43e254499bb2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define the two SMILES strings\n",
    "smiles1 = \"O=C(N[Dy])c1cccc(F)c1Nc1nc(Nc2ccc(Cn3ccnc3)cc2)nc(Nc2cc(Cl)c(O)c(Cl)c2)n1\"\n",
    "smiles2 = \"O=C(N[Dy])c1cccc(F)c1Nc1nc(Nc2ccc(Cn3ccnc3)cc2)nc(Nc2cc(Cl)c(F)c(Cl)c2)n1\"\n",
    "\n",
    "# Convert SMILES to molecule objects\n",
    "mol1 = Chem.MolFromSmiles(smiles1)\n",
    "mol2 = Chem.MolFromSmiles(smiles2)\n",
    "\n",
    "# Generate the Morgan fingerprints\n",
    "fp1 = AllChem.GetMorganFingerprintAsBitVect(mol1, radius=2, nBits=512)\n",
    "fp2 = AllChem.GetMorganFingerprintAsBitVect(mol2, radius=2, nBits=512)\n",
    "\n",
    "# Calculate the Tanimoto similarity between the two fingerprints\n",
    "similarity = TanimotoSimilarity(fp1, fp2)\n",
    "print(f\"The Tanimoto similarity between the fingerprints is: {similarity}\")"
   ],
   "id": "c233c22c95d3b761",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Testing Out Data Retrieval",
   "id": "b0a3afc4bfb35a0f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def prepare_data(cursor):\n",
    "    \"\"\"Prepare training and testing datasets.\"\"\"\n",
    "    print('Cursor received by prepare_data()...')\n",
    "\n",
    "    # Create a temporary table to store training IDs\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE TEMPORARY TABLE IF NOT EXISTS temp_train_ids AS\n",
    "        SELECT id FROM compounds WHERE binds = 1 ORDER BY id LIMIT 1000000;\n",
    "    \"\"\")\n",
    "    print('Temporary table for training IDs created...')\n",
    "\n",
    "    # Fetch the training data using the temporary table\n",
    "    query_train = \"\"\"\n",
    "        SELECT c.id, c.molecule_smiles, c.protein_name, c.binds \n",
    "        FROM compounds AS c\n",
    "        JOIN temp_train_ids AS t ON c.id = t.id;\n",
    "    \"\"\"\n",
    "    cursor.execute(query_train)\n",
    "    train_pos = cursor.fetchall()\n",
    "    print('Training data prepared for binds = 1...')\n",
    "\n",
    "    # Fetch the testing data by excluding IDs in the temporary table\n",
    "    query_test_positive = \"\"\"\n",
    "        SELECT id, molecule_smiles, protein_name, binds \n",
    "        FROM compounds \n",
    "        WHERE binds = 1 AND id NOT IN (SELECT id FROM temp_train_ids);\n",
    "    \"\"\"\n",
    "    cursor.execute(query_test_positive)\n",
    "    test_pos = cursor.fetchall()\n",
    "    print('Testing data prepared for binds = 1...')\n",
    "\n",
    "    # Count for negative instances\n",
    "    cursor.execute(\"SELECT COUNT(*) FROM compounds WHERE binds = 0\")\n",
    "    total_neg = cursor.fetchone()[0]\n",
    "    neg_sample_size = len(test_pos)  # Ensures balance in the test set\n",
    "    print(f\"Negative sample size for testing determined: {neg_sample_size}...\")\n",
    "\n",
    "    # Fetch negative instances for train and test\n",
    "    query_neg_train = \"\"\"\n",
    "        SELECT id, molecule_smiles, protein_name, binds FROM compounds WHERE binds = 0 LIMIT 1000000;\n",
    "    \"\"\"\n",
    "    cursor.execute(query_neg_train)\n",
    "    train_neg = cursor.fetchall()\n",
    "    print('Training data prepared for binds = 0...')\n",
    "\n",
    "    query_neg_test = f\"\"\"\n",
    "        SELECT id, molecule_smiles, protein_name, binds FROM compounds WHERE binds = 0 LIMIT {neg_sample_size} OFFSET 1000000;\n",
    "    \"\"\"\n",
    "    cursor.execute(query_neg_test)\n",
    "    test_neg = cursor.fetchall()\n",
    "    print('Testing data prepared for binds = 0...')\n",
    "\n",
    "    # Cleanup: drop the temporary table\n",
    "    cursor.execute(\"DROP TEMPORARY TABLE IF EXISTS temp_train_ids;\")\n",
    "    print('Temporary table dropped...')\n",
    "\n",
    "    # Combine and shuffle datasets\n",
    "    train_data = np.vstack((train_pos, train_neg))\n",
    "    test_data = np.vstack((test_pos, test_neg))\n",
    "    np.random.shuffle(train_data)\n",
    "    np.random.shuffle(test_data)\n",
    "    print('Data sets shuffled and ready to be returned...')\n",
    "\n",
    "    return train_data, test_data"
   ],
   "id": "d57ecfbe70a7d5c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def evaluate_model(X, y, clf):\n",
    "    \"\"\"Evaluate the model with confusion matrix and classification report.\"\"\"\n",
    "    predictions = clf.predict(X)\n",
    "    conf_matrix = confusion_matrix(y, predictions)\n",
    "    classif_report = classification_report(y, predictions)\n",
    "    accuracy = accuracy_score(y, predictions)\n",
    "    return conf_matrix, classif_report, accuracy"
   ],
   "id": "ed066fd3d6248713",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "conn = connect_to_database()\n",
    "try:\n",
    "    cursor = conn.cursor()\n",
    "    # Assume `prepare_data` fetches and prepares training and testing data\n",
    "    train_data, test_data = prepare_data(cursor)\n",
    "except Error as e:\n",
    "    print(f\"The error '{e}' occurred\")\n",
    "finally:\n",
    "    close_connection(conn)\n"
   ],
   "id": "61d63a6a056f1e0a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# conn = connect_to_database()\n",
    "# try:\n",
    "#     cursor = conn.cursor()\n",
    "#     # Assume `prepare_data` fetches and prepares training and testing data\n",
    "#     train_and_evaluate(train_data, test_data)\n",
    "# except Error as e:\n",
    "#     print(f\"The error '{e}' occurred\")\n",
    "# finally:\n",
    "#     close_connection(conn)\n"
   ],
   "id": "5dba5c3eb3eaa1e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_data = pd.DataFrame(test_data)\n",
    "train_data = pd.DataFrame(train_data)"
   ],
   "id": "4376438b70d8d11a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_data.columns = ['ID', 'Mol_SMILES', 'Protein', 'Binds']\n",
    "train_data.columns = ['ID', 'Mol_SMILES', 'Protein', 'Binds']"
   ],
   "id": "861c9e75ed270fe1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(train_data['Mol_SMILES'])",
   "id": "bf3f71f87b4ab137",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def smiles_to_fingerprint(smiles, radius=2, nBits=512):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is not None:\n",
    "        return list(AllChem.GetMorganFingerprintAsBitVect(mol, radius=radius, nBits=nBits))\n",
    "    else:\n",
    "        return [0]*nBits  # Return a zero vector if the molecule parsing fail\n",
    "def process_in_batches(smiles_series, batch_size=1000):\n",
    "    \"\"\"Process Series in batches to convert SMILES to fingerprints.\"\"\"\n",
    "    num_batches = (len(smiles_series) + batch_size - 1) // batch_size  # Calculate number of batches\n",
    "    results = []  # This will store the final results\n",
    "\n",
    "    for i in tqdm(range(num_batches), desc=\"Processing Batches\"):\n",
    "        batch = smiles_series.iloc[i*batch_size:(i+1)*batch_size].apply(smiles_to_fingerprint)\n",
    "        results.extend(batch)  # Append results directly\n",
    "\n",
    "    return pd.Series(results)  # Return as a Series\n"
   ],
   "id": "131f210a0f31094f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_df = pd.DataFrame()\n",
    "train_df = pd.DataFrame()\n",
    "\n",
    "train_df['id'] = train_data['ID']\n",
    "test_df['id'] = test_data['ID']\n",
    "train_df['binds'] = train_data['Binds']\n",
    "test_df['binds'] = test_data['Binds']"
   ],
   "id": "3a8d14abe124dede",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Applying the function to the SMILES column\n",
    "test_df['molVector'] = process_in_batches(test_data['Mol_SMILES'], batch_size=500)\n",
    "train_df['molVector'] = process_in_batches(train_data['Mol_SMILES'], batch_size=500)"
   ],
   "id": "16a6cd12ce26ad3f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def fetch_protein_mapping(cursor):\n",
    "    \"\"\"Fetch protein name to numeric mapping from the database.\"\"\"\n",
    "    query = \"SELECT protein_name, protein_numeric FROM protein_mapping\"\n",
    "    cursor.execute(query)\n",
    "    mapping_data = cursor.fetchall()\n",
    "    print('fetched protein mapping...')\n",
    "    return {name: num for name, num in mapping_data}\n",
    "\n",
    "conn = connect_to_database()\n",
    "cursor = conn.cursor()\n",
    "protein_mapping = fetch_protein_mapping(cursor)"
   ],
   "id": "7df85b220a6bdf5d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_df['Protein_numeric'] = train_data['Protein'].map(protein_mapping)\n",
    "test_df['Protein_numeric'] = test_data['Protein'].map(protein_mapping) "
   ],
   "id": "1a9ffdde13fe1dd6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "del train_data\n",
    "del test_data\n",
    "gc.collect()"
   ],
   "id": "f2a5e1e91e727426",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# train_df.to_csv('train_data.csv', index=False)\n",
    "# test_df.to_csv('test_data.csv', index=False)"
   ],
   "id": "eac47eba71a5bebd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_df = pd.read_csv('train_data.csv')\n",
    "test_df = pd.read_csv('test_data.csv')"
   ],
   "id": "906f16f3dee37734",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X_train = train_df.drop(['id', 'binds'], axis=1)\n",
    "y_train = train_df['binds']\n",
    "\n",
    "X_test = test_df.drop(['id', 'binds'], axis=1)\n",
    "y_test = test_df['binds']"
   ],
   "id": "d00696b51b7543fe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(X_train.dtypes)\n",
    "print(X_train.columns)"
   ],
   "id": "8797573afd0dbec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Assuming train_df and test_df are already defined and loaded\n",
    "\n",
    "# Initialize OneHotEncoder for categorical variable transformation\n",
    "encoder = OneHotEncoder(sparse=True)  # Output dense format\n",
    "print('generating encoder vals...')\n",
    "# Fit the encoder on the combined 'Protein_numeric' data (from both train and test)\n",
    "combined_protein_numeric = pd.concat([train_df['Protein_numeric'], test_df['Protein_numeric']])\n",
    "encoder.fit(combined_protein_numeric.values.reshape(-1, 1))\n",
    "print('applying vals to dataframes...')\n",
    "# Transform 'Protein_numeric' for both training and testing datasets\n",
    "protein_onehot_train = encoder.transform(train_df['Protein_numeric'].values.reshape(-1, 1))\n",
    "protein_onehot_test = encoder.transform(test_df['Protein_numeric'].values.reshape(-1, 1))\n",
    "print('zipping files...')\n",
    "# Concatenate 'molVector' with one-hot encoded protein vectors\n",
    "X_train = [list(mv) + list(ph) for mv, ph in zip(train_df['molVector'].tolist(), protein_onehot_train)]\n",
    "X_test = [list(mv) + list(ph) for mv, ph in zip(test_df['molVector'].tolist(), protein_onehot_test)]\n",
    "\n",
    "# Directly use 'binds' column for targets\n",
    "y_train = train_df['binds'].tolist()\n",
    "y_test = test_df['binds'].tolist()"
   ],
   "id": "84cb4640f7dc3dc0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "del train_df\n",
    "del test_df\n",
    "gc.collect()"
   ],
   "id": "59da09044767b235",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from xgboost import XGBClassifier\n",
    "# Create the XGBoost classifier\n",
    "model = XGBClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "\n",
    "model.fit(X_train, y_train)"
   ],
   "id": "150e280b54d7a290",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "predictions = model.predict(X_test)\n",
    "probabilities = model.predict_proba(X_test)[:,1]"
   ],
   "id": "e174b3d5a8869798",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
    "print(\"Precision:\", precision_score(y_test, predictions))\n",
    "print(\"Recall:\", recall_score(y_test, predictions))\n",
    "print(\"F1 Score:\", f1_score(y_test, predictions))\n",
    "\n",
    "\n",
    "\n",
    "print(classification_report(y_test, predictions, target_names=['Class 0', 'Class 1']))\n",
    "\n",
    "\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, predictions)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, probabilities)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "print(\"AUC Score:\", roc_auc)"
   ],
   "id": "6287b989869e1e84",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "bf13eb5230a39642",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
